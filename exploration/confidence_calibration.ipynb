{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence calibration evluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from anomaly_scores.vim_scores import VIM\n",
    "from energy_ood.CIFAR.models.wrn import WideResNet\n",
    "\n",
    "from energy_ood.utils.svhn_loader import SVHN\n",
    "from util.confidence import compute_rms_calibration_error\n",
    "from util.get_ood_score import to_np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We are using the Wide ResNet as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideResNet(depth=40, num_classes=10, widen_factor=2, dropRate=0.3)\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"energy_ood/CIFAR/snapshots/pretrained/cifar10_wrn_pretrained_epoch_99.pt\"\n",
    "    )\n",
    ")\n",
    "model.eval()\n",
    "_ = model.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and standard deviation of channels of CIFAR-10 images\n",
    "torch.cuda.empty_cache()\n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "test_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(mean, std)]\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\"data/cifar10\", train=False, transform=test_transform)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=2500, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "vim = VIM(data_loader, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On vanilla model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_vanilla = []\n",
    "error_vim = []\n",
    "for input, labels in data_loader:\n",
    "    if torch.cuda.is_available():\n",
    "        input = input.cuda()\n",
    "    logits, penultimate = model(input)\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    error_vanilla.append(compute_rms_calibration_error(to_np(labels), to_np(probabilities)))\n",
    "\n",
    "    virtual_logit = vim.compute_anomaly_score(logits, penultimate)\n",
    "    virutal_logits = torch.hstack((logits.cpu(), torch.from_numpy(np.expand_dims(virtual_logit, axis=1))))\n",
    "    virutal_probabilities = torch.nn.functional.softmax(virutal_logits, dim=-1)\n",
    "    error_vim.append(compute_rms_calibration_error(to_np(labels), to_np(virutal_probabilities[:, :10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04857253001692445 0.006443243636239018\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(error_vanilla),np.std(error_vanilla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05959132114844886 0.0017581347136628384\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(error_vim),np.std(error_vim))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so vim does not improve it. It might even be worse. But the reason could be, that the probabilities are not fully distributed over the entire space. That's why we have some bins around low calibration with just a few samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With OOD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_data = SVHN(\n",
    "    root=\"data/svhn/\",\n",
    "    split=\"test\",\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize(mean, std)]  # trn.Resize(32),\n",
    "    ),\n",
    "    download=False,\n",
    ")\n",
    "ood_loader = torch.utils.data.DataLoader(\n",
    "    ood_data, batch_size=2000, shuffle=True, num_workers=2, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_vanilla = []\n",
    "error_vim = []\n",
    "for input, labels in ood_loader:\n",
    "    if torch.cuda.is_available():\n",
    "        input = input.cuda()\n",
    "    logits, penultimate = model(input)\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    error_vanilla.append(compute_rms_calibration_error(to_np(labels), to_np(probabilities)))\n",
    "\n",
    "    virtual_logit = vim.compute_anomaly_score(logits, penultimate)\n",
    "    virutal_logits = torch.hstack((logits.cpu(), torch.from_numpy(np.expand_dims(virtual_logit, axis=1))))\n",
    "    virutal_probabilities = torch.nn.functional.softmax(virutal_logits, dim=-1)\n",
    "    error_vim.append(compute_rms_calibration_error(to_np(labels), to_np(virutal_probabilities[:, :10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6884658125758784 0.010476772860678628\n",
      "0.5916646976370383 0.008349233911948351\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(error_vanilla),np.std(error_vanilla))\n",
    "print(np.mean(error_vim),np.std(error_vim))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's how I expected it! Improvement of around 10 %-points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "activation-ood-VPh-MuUw-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
